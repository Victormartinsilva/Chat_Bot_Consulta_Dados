import streamlit as st
from chatbot import gerar_resposta
import pandas as pd
import os

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="Chatbot de Consulta de Dados (LangChain/Pandas)", layout="wide")

# T√≠tulo e descri√ß√£o
st.title("ü§ñ Chatbot de Consulta de Dados")
st.markdown(
    """
    Este chatbot utiliza **LangChain** e **Pandas** para responder a perguntas
    sobre o seu arquivo de dados (`data.csv`).
    
    **Suporta m√∫ltiplos provedores:** OpenAI, Ollama (gratuito/local), Google Gemini (gratuito)
    
    Fa√ßa perguntas em linguagem natural sobre os dados, e o agente ir√° gerar e executar
    o c√≥digo Python necess√°rio para obter a resposta.
    
    **Configure o provedor no arquivo `.env` usando a vari√°vel `LLM_PROVIDER`**
    """
)

# Fun√ß√£o para obter secrets (suporta Streamlit Cloud e .env local)
def get_secret(key, default=None):
    """Obt√©m secret do Streamlit Cloud ou vari√°vel de ambiente local"""
    try:
        # Tenta obter do Streamlit secrets primeiro (Streamlit Cloud)
        if hasattr(st, 'secrets') and key in st.secrets:
            return st.secrets[key]
    except:
        pass
    # Fallback para vari√°vel de ambiente (.env local)
    return os.getenv(key, default)

# Mostra qual provedor est√° sendo usado
llm_provider = get_secret("LLM_PROVIDER", "openai").upper()
st.info(f"üîß Provedor LLM configurado: **{llm_provider}**")

# --- Carregamento e Exibi√ß√£o do DataFrame ---
CSV_FILE_PATH = "data.csv"
# Suporte para CSV via URL (√∫til para Streamlit Cloud)
CSV_URL = get_secret("CSV_URL", None)

try:
    # Tenta carregar de URL primeiro (para Streamlit Cloud), depois do arquivo local
    if CSV_URL:
        df = pd.read_csv(CSV_URL)
        st.success(f"‚úÖ CSV carregado de URL: {CSV_URL}")
    else:
        df = pd.read_csv(CSV_FILE_PATH)
        st.success(f"‚úÖ CSV carregado do arquivo local: {CSV_FILE_PATH}")
    st.subheader("Amostra do DataFrame Carregado")
    st.dataframe(df.head())
    st.info(f"DataFrame carregado com sucesso: {df.shape[0]} linhas e {df.shape[1]} colunas.")
except FileNotFoundError:
    st.error(f"Erro: Arquivo CSV n√£o encontrado em {CSV_FILE_PATH}. Certifique-se de que 'data.csv' est√° no diret√≥rio correto.")
    st.stop()
except Exception as e:
    st.error(f"Erro ao carregar o DataFrame: {e}")
    st.stop()

# --- Inicializa√ß√£o do Hist√≥rico de Conversa ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "Ol√°! Eu sou um agente de dados. Pergunte-me algo sobre o DataFrame acima!"})

if "raciocinios" not in st.session_state:
    st.session_state.raciocinios = {}

# --- Exibi√ß√£o do Hist√≥rico de Conversa ---
for i, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"]):
        # Verifica se √© uma mensagem de erro
        if message["content"].startswith(("‚ö†Ô∏è", "üîë", "‚ùå")):
            st.error(message["content"])
        else:
            st.markdown(message["content"])
    
    # Exibe o racioc√≠nio fora do chat_message para evitar problemas de renderiza√ß√£o
    if message["role"] == "assistant" and i in st.session_state.raciocinios:
        raciocinio = st.session_state.raciocinios[i]
        if raciocinio and raciocinio.strip():
            with st.expander("üîç Racioc√≠nio (C√≥digo Python Gerado)", expanded=False):
                st.code(raciocinio, language="python")

# --- Entrada do Usu√°rio ---
if prompt := st.chat_input("Digite sua pergunta sobre os dados..."):
    # 1. Adiciona a mensagem do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # 2. Renderiza a mensagem do usu√°rio imediatamente
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # 3. Gera a resposta
    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            try:
                resposta_final, raciocinio = gerar_resposta(prompt)
            except Exception as e:
                resposta_final = f"‚ùå **Erro inesperado:** {str(e)}"
                raciocinio = ""
        
        # Verifica se √© uma mensagem de erro
        if resposta_final.startswith(("‚ö†Ô∏è", "üîë", "‚ùå")):
            st.error(resposta_final)
        else:
            st.markdown(resposta_final)
        
        # Exibe o racioc√≠nio se houver
        if raciocinio and raciocinio.strip():
            with st.expander("üîç Racioc√≠nio (C√≥digo Python Gerado)", expanded=False):
                st.code(raciocinio, language="python")
    
    # 4. Adiciona a resposta do assistente ao hist√≥rico
    indice_resposta = len(st.session_state.messages)
    st.session_state.messages.append({"role": "assistant", "content": resposta_final})
    
    # 5. Armazena o racioc√≠nio se houver
    if raciocinio and raciocinio.strip():
        st.session_state.raciocinios[indice_resposta] = raciocinio

# --- Aviso sobre a Chave da API ---
llm_provider = get_secret("LLM_PROVIDER", "openai").lower()
if llm_provider == "openai" and not get_secret("OPENAI_API_KEY"):
    st.warning("‚ö†Ô∏è A chave `OPENAI_API_KEY` n√£o foi encontrada. Configure nos Secrets do Streamlit Cloud ou no arquivo `.env`")
elif llm_provider == "gemini" and not get_secret("GOOGLE_API_KEY"):
    st.warning("‚ö†Ô∏è A chave `GOOGLE_API_KEY` n√£o foi encontrada. Configure nos Secrets do Streamlit Cloud ou obtenha uma em: https://makersuite.google.com/app/apikey")
elif llm_provider == "ollama":
    st.info("‚úÖ Usando Ollama (gratuito). Certifique-se de que o Ollama est√° rodando: `ollama serve`")
